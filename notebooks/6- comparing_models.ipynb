{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import logging\n",
    "import pickle\n",
    "import random\n",
    "import plotly \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline, FeatureUnion\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"job-profile-prediction\"\n",
    "TRACKING_URI = \"../mlflow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for run 92ed66d742664ef78606066f1ed74a8c (Model: xgboost):\n",
      "precision_score: 98.99234042553194\n",
      "accuracy_score: 99.5314893617021\n",
      "recall_score: 82.05170212765957\n",
      "f1_score: 88.53914893617019\n",
      "\n",
      "Metrics for run a658fec9c539468c8e7115668a53c5e6 (Model: logistic_regression):\n",
      "precision_score: 62.15723404255319\n",
      "accuracy_score: 98.13489361702128\n",
      "recall_score: 37.32\n",
      "f1_score: 44.01595744680852\n",
      "\n",
      "Metrics for run 3634e7de16c4427a9d60d615f2ae940f (Model: logistic_regression):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of MlflowClient\n",
    "client = MlflowClient(tracking_uri=TRACKING_URI)\n",
    "\n",
    "# Get the experiment ID based on the experiment name\n",
    "experiment_id = client.get_experiment_by_name(EXPERIMENT_NAME).experiment_id\n",
    "\n",
    "# Get the runs information for the experiment\n",
    "runs = client.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "# Create an empty dictionary to store the metrics\n",
    "metrics_dict = {}\n",
    "\n",
    "# Iterate over the runs\n",
    "for run in runs:\n",
    "    # Get the run ID\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # Get the metrics for the run\n",
    "    metrics = client.get_run(run_id).data.metrics\n",
    "    \n",
    "    # Get the model name for the run\n",
    "    model_name = client.get_run(run_id).data.tags.get(\"mlflow.runName\")\n",
    "    \n",
    "    # Store the metrics in the dictionary\n",
    "    metrics_dict[run_id] = (model_name, metrics)\n",
    "\n",
    "# Print the metrics and model name for each run\n",
    "for run_id, (model_name, metrics) in metrics_dict.items():\n",
    "    print(f\"Metrics for run {run_id} (Model: {model_name}):\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type NoneType doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Aya/Tech-Job-profile-prediction-dsProject/notebooks/6- comparing_models.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Aya/Tech-Job-profile-prediction-dsProject/notebooks/6-%20comparing_models.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m run_id, (model_name, metrics) \u001b[39min\u001b[39;00m metrics_dict\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Aya/Tech-Job-profile-prediction-dsProject/notebooks/6-%20comparing_models.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model_names\u001b[39m.\u001b[39mappend(model_name)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Aya/Tech-Job-profile-prediction-dsProject/notebooks/6-%20comparing_models.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     recall_score\u001b[39m.\u001b[39mappend(\u001b[39mround\u001b[39m(metrics\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrecall_score\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m2\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Aya/Tech-Job-profile-prediction-dsProject/notebooks/6-%20comparing_models.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     precision_scores\u001b[39m.\u001b[39mappend(\u001b[39mround\u001b[39m(metrics\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mprecision_score\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m2\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Aya/Tech-Job-profile-prediction-dsProject/notebooks/6-%20comparing_models.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     f1_scores\u001b[39m.\u001b[39mappend(\u001b[39mround\u001b[39m(metrics\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m\"\u001b[39m) ,\u001b[39m2\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: type NoneType doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the model names and metrics from the dictionary\n",
    "model_names = []\n",
    "recall_score = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "\n",
    "for run_id, (model_name, metrics) in metrics_dict.items():\n",
    "    model_names.append(model_name)\n",
    "    recall_score.append(round(metrics.get(\"recall_score\"), 2))\n",
    "    precision_scores.append(round(metrics.get(\"precision_score\"), 2))\n",
    "    f1_scores.append(round(metrics.get(\"f1_score\") ,2))\n",
    "\n",
    "\n",
    "# Set the x-axis positions\n",
    "x = range(len(model_names))\n",
    "\n",
    "# Set the width of each bar\n",
    "bar_width = 0.15\n",
    "\n",
    "# Plot the accuracy scores\n",
    "plt.bar(x, recall_score, width=bar_width, label=\"Recall Score\")\n",
    "\n",
    "# Plot the Precision scores\n",
    "plt.bar([i + bar_width for i in x], precision_scores, width=bar_width, label=\"Precision Score\")\n",
    "\n",
    "# Plot the F1 scores\n",
    "plt.bar([i + 2*bar_width for i in x], f1_scores, width=bar_width, label=\"F1 Score\")\n",
    "\n",
    "# Set the x-axis labels\n",
    "plt.xticks([i + bar_width for i in x], model_names, rotation=45)\n",
    "\n",
    "# Set the y-axis label\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "# Set the chart title\n",
    "plt.title(\"Comparison of Metrics for Each Run\")\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
